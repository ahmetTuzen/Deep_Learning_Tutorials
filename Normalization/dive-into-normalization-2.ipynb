{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b2df4b",
   "metadata": {
    "papermill": {
     "duration": 0.005165,
     "end_time": "2023-09-06T17:22:06.271386",
     "exception": false,
     "start_time": "2023-09-06T17:22:06.266221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hello everyone, in this mini notebook, we will examine some important normalization techniques topics.\n",
    "\n",
    "### I hope you will remember these topics or even learn some!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5408d44",
   "metadata": {
    "papermill": {
     "duration": 0.004169,
     "end_time": "2023-09-06T17:22:06.280269",
     "exception": false,
     "start_time": "2023-09-06T17:22:06.276100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Table of Contents - Part 2\n",
    "\n",
    "[Part 1: Introduction, Min-max normalization, Z-score normalization, Robust normalization, Power normalization](https://www.kaggle.com/code/atuzen/dive-into-normalization/)\n",
    "1. [Batch normalization](#1)\n",
    "    1. [Definition](#1.1)\n",
    "    1. [When to apply/add?](#1.2)\n",
    "    1. [Benefits](#1.3)\n",
    "1. [Layer normalization](#2)\n",
    "    1. [Introduction](#2.1)\n",
    "    1. [Benefits](#2.2)\n",
    "    1. [Layer normalization vs batch normalization](#2.3)\n",
    "1. [Group normalization](#3)\n",
    "    1. [Definition](#3.1)\n",
    "    1. [Benefits](#3.2)\n",
    "    1. [Normalization methods comparison](#3.3)\n",
    "1. [Local response normalization](#4)\n",
    "    1. [Definition](#4.1)\n",
    "1. [Conclusion](#5)\n",
    "    1. [References and links](#5.1)\n",
    "\n",
    "\n",
    "**Note: The link to Part 1 is provided with a link, as these topics are less advanced compared to these normalization techniques.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7573bc4",
   "metadata": {
    "papermill": {
     "duration": 0.004215,
     "end_time": "2023-09-06T17:22:06.289123",
     "exception": false,
     "start_time": "2023-09-06T17:22:06.284908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Batch normalization <a id=1></a>\n",
    "\n",
    "## Definition <a id=1.1></a>\n",
    "\n",
    "#### Batch normalization is introduced in paper written by Sergey Ioffe and Christian Szegedy in their paper \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\" [link](https://arxiv.org/abs/1502.03167). [1] This technique is proposed for training deep neural networks (DNN).\n",
    "\n",
    "#### The problem with DNN is that the input of each layers is constantly changing, which slows down the training phase and causing adopting smaller learning rates usually.\n",
    "\n",
    "#### Batch normalization addresses this problem by performing normalization to each batch. And thanks to batch normalization, higher learning rates as well as the dependency of the dropout regularization is reduced.\n",
    "\n",
    "#### This normalization operation makes each dimension unit gaussians ($\\mu \\rightarrow 0, \\sigma \\rightarrow 1$) for each batch which makes the model more stable.\n",
    "\n",
    "## When to apply/add? <a id=1.2></a> \n",
    "\n",
    "#### Usually insert after convolutional layer(s) and/or fully connected layer(s).\n",
    "\n",
    "## Benefits <a id=1.3></a> \n",
    "\n",
    "* Stable and faster training (improves gradient flow).\n",
    "* Regularization technique (you can check my notebook for regularization here).\n",
    "* Allowing higher learning rates.\n",
    "* Dependence of weight initialization is reduced.\n",
    "* The risk of vanishing or exploiting gradients are reduced (therefore deeper networks are possible).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d1523",
   "metadata": {
    "papermill": {
     "duration": 0.00415,
     "end_time": "2023-09-06T17:22:06.297658",
     "exception": false,
     "start_time": "2023-09-06T17:22:06.293508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Layer normalization <a id=2></a>\n",
    "\n",
    "## Introduction <a id=2.1></a>\n",
    "\n",
    "#### Layer normalization is introduced in paper \"Layer Normalization\" written by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton [link](https://arxiv.org/abs/1607.06450) [2].\n",
    "\n",
    "#### This technique is introduced to address some limitations of batch normalization. \n",
    "\n",
    "#### Layer normalization normalizes the input for each layer independently. \n",
    "\n",
    "## Benefits <a id=2.2></a>\n",
    "\n",
    "#### The benefits of batch normalization is also apply to layer normalization.\n",
    "\n",
    "#### Additionally, Layer normalization is less dependent on mini-batch size.\n",
    "\n",
    "## Layer normalization vs batch normalization <a id=2.3></a>\n",
    "\n",
    "#### Batch normalization normalizes within mini-batches and layer normalization normalizes across all features.\n",
    "\n",
    "#### To summarize, batch normalization operates horizontally and layer norm operates vertically.\n",
    "\n",
    "#### Layer normalization is more suitable for RNN's, while batch normalization is used for CNN and deep feedforward networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798fcd3",
   "metadata": {
    "papermill": {
     "duration": 0.004211,
     "end_time": "2023-09-06T17:22:06.306324",
     "exception": false,
     "start_time": "2023-09-06T17:22:06.302113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Group normalization <a id=3></a>\n",
    "\n",
    "## Definition <a id=3.1></a>\n",
    "\n",
    "#### Group normalization is introduced in paper \"Group Normalization\" written by Yuxin Wu and Kaiming He [link](https://arxiv.org/abs/1803.08494) [3].\n",
    "\n",
    "#### This technique is introduced to address some limitations of batch normalization when the batch sizes become smaller.\n",
    "\n",
    "#### Group normalization divides input channels into smaller groups and then normalizes these groups independently.\n",
    "\n",
    "## Benefits <a id=3.2></a>\n",
    "\n",
    "#### The benefits of batch normalization is also apply to group normalization.\n",
    "\n",
    "#### Group normalization is not highly dependent on batch size.\n",
    "\n",
    "#### It also has the benefits oflayer normalization, meaning that group normalization is a good choice for CNNs and RNNs.\n",
    "\n",
    "## Normalization methods comparison <a id=3.3></a>\n",
    "\n",
    "**Image is taken from paper [3]**\n",
    "\n",
    "#### I am providing this image directly from the paper as the image makes it much easier to understand.\n",
    "\n",
    "![](https://github.com/ahmetTuzen/Deep_Learning_Tutorials/blob/main/Normalization/normalization%20methods.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ca4d8",
   "metadata": {
    "papermill": {
     "duration": 0.004165,
     "end_time": "2023-09-06T17:22:06.315147",
     "exception": false,
     "start_time": "2023-09-06T17:22:06.310982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Local response normalization <a id=4></a>\n",
    "\n",
    "## Definition <a id=4.1></a>\n",
    "\n",
    "#### Local response normalization is introduced in the famous AlexNet [4] architecture.\n",
    "\n",
    "#### It is based on the calculation of a normalization factor from the local neighborhood that used for this technique.\n",
    "\n",
    "#### Strong neurons are more effective in this normalization as they are impacting the neighbors.\n",
    "\n",
    "#### Unlike the first three normalizations, there is no vertical, horizontal or grouping, normalization is operated for each feature individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96e8de",
   "metadata": {
    "papermill": {
     "duration": 0.004039,
     "end_time": "2023-09-06T17:22:06.323540",
     "exception": false,
     "start_time": "2023-09-06T17:22:06.319501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion <a id=5></a>\n",
    "\n",
    "#### In these two mini-notebooks, we examined normalization techniques that are widely adopted by machine learning and deep learning models and applications. I hope you have enjoyed these notebooks. If you have any questions or would like to add anything, I would love to hear them.\n",
    "\n",
    "#### Finally, I appreacte any feedback/upvotes.\n",
    "\n",
    "## References and links <a id=5.1></a>\n",
    "\n",
    "##### Papers:\n",
    "\n",
    "[1] Ioffe, S., & Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). pmlr.\n",
    "\n",
    "[2] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n",
    "\n",
    "[3] Wu, Y., & He, K. (2018). Group normalization. In Proceedings of the European conference on computer vision (ECCV) (pp. 3-19).\n",
    "\n",
    "[4] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25.\n",
    "\n",
    "#### Normalization layers in pytorch: [link](https://pytorch.org/docs/stable/nn.html#normalization-layers)\n",
    "\n",
    "#### My links: [GitHub](https://github.com/ahmetTuzen/Deep_Learning_Tutorials) and [LinkedIn](https://www.linkedin.com/in/ahmet-tuzen/)"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.6.4"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.108278,
   "end_time": "2023-09-06T17:22:06.647398",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-06T17:22:02.539120",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
